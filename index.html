<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shengjun Tang</title>
  
  <meta name="author" content="Shengjun Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌐</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shengjun Tang</name>
              </p>
              <p> Dr. Shengjun Tang is currently an Assistant Professor/Associate Researcher at <a href="https://www.szu.edu.cn/">Shenzhen University</a>. Prior to this, he worked as a Research Assistant at <a href="https://www.polyu.edu.hk/lsgi/"> the Department of Land Surveying and Geo-Informatics (LSGI) at The Hong Kong Polytechnic University</a>, and as a Postdoctoral Researcher in the team of Academician Renzhong Guo at Shenzhen University.
	      </p>
              <p>His research interests include 3D reconstruction and scene understanding, laser and visual simultaneous localization and mapping (SLAM), Virtual Geography and modeling theory, and applications. 
              </p>
              <p style="text-align:center">
                <a href="mailto:shengjuntang@szu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV_ShengjunTang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q-fQQqUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/shiyujiao">Github</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Shengjun%20tang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ShengjuntangCircle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2022: I start an internship at Tencent</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2022: One paper accepted to TPAMI 2022 </li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2022: One paper accepted to CVPR 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2022: I start an internship at Meta Reality Lab Research</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">12/2021: One paper accepted to TPAMI 2022</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2021: I am nominated as one of the CVPR 2021 outstanding reviewer</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2021: One paper accepted to CVPR 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2020: One paper accepted to CVPR 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2019: One paper accepted to AAAI 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2019: One paper accepted to NeurIPS 2019</li>

                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table> -->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treemodeling.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2">
                <papertitle>面向虚拟地理环境构建的树木模型高保真三维重建方法 </papertitle>
              </a>
              <br>
              王伟玺, 黄鸿盛,杜思齐, 李晓明, 谢林甫, 郭仁忠, <strong>汤圣君*</strong>
              <br>
              遥感学报, 2023
              <br>
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2"> paper </a> 
              <p></p>
              <p> 本文面向虚拟地理环境高逼真场景构建需求，提出一种基于高精度激光扫描点云数据的树木三维模型高保真仿生重建方法，可实现形态特征保持的树木三维模型自动化重建。
		      
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorclassification.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82">
                <papertitle>超体素随机森林与 LSTM 神经网络联合优化的室内点云高精度分类方法 </papertitle>
              </a>
              <br>
              <strong>汤圣君</strong>, 张韵婕, 李晓明, 姚萌萌, 叶致煌, 李亚鑫, 郭仁忠, 王伟玺
              <br>
              武汉大学学报（信息科学版）, 2023
              <br>
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82"> paper </a>
              <p></p>
              <p> 针对现有三维点云数据分割分类方法存在分类目标内部不一致的问题，提出一种超体素随机森林与长短期记忆神经网络(long short-term memory，LSTM)联合优化的室内点云高精度分类方法。
		      
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/jag_pond.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp">
                <papertitle>Unsupervised stepwise extraction of offshore aquaculture ponds using super-resolution hyperspectral images </papertitle>
              </a>
              <br>
	      Siqi Du, Hongsheng Huang, Fan He, Heng Luo, Yumeng Yin, Xiaoming Li, Linfu Xie, Renzhong Guo,  <strong>Shengjun Tang*</strong>
              <br>
              International Journal of Applied Earth Observation and Geoinformation, 2023 (SCI, Top)
              <br>
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp"> paper </a> 
              <p></p>
              <p> In this paper, we proposed an unsupervised aquaculture ponds extraction method based on hyperspectral imagery super-resolution, feature fusion and stepwise extraction strategy. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treesegJSTAR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735">
                <papertitle>An Individual Tree Segmentation Method from Mobile Mapping Point Clouds Based on Improved 3D Morphological Analysis</papertitle>
              </a>
              <br>
             Weixi Wang, Yuhang Fan, You Li, Xiaoming Li, <strong>Shengjun Tang*</strong>
              <br>
              IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2023
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735"> paper </a> 
              <p></p>
              <p> We propose a method based on improved 3D morphological analysis for extracting street trees from mobile laser scanner (MLS) point clouds. 
              </p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SVNVS.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o">
                <papertitle>BIM generation from 3D point clouds by combining 3D deep learning and improved morphological approach</papertitle>
              </a>
              <br>
             <strong> Shengjun Tang</strong>, Xiaoming Li, Xianwei Zheng, Bo Wu, Weixi Wang, Yunjie Zhang
              <br>
              Automation In Construction, 2022 (SCI, Top)
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o"> paper </a> 
              <p></p>
              <p> This paper presents a novel parametric modeling method for reconstructing semantic volumetric building interiors from the unstructured point cloud of a building. Unlike existing partitioning-based methods, our proposed method overcomes the limitations by providing a flexible framework for combining 3D Deep Learning and an improved morphological approach for inverse BIM modeling. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/DSM.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf">
                <papertitle>Where am I looking at? Joint Location and Orientation Estimation by Cross-View Matching </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Dylan Campbell, and Hongdong Li
              <br>
              CVPR, 2020
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_DSM"> code </a>
              <p></p>
              <p> The first 3-DoF camera pose estimation framework via ground-to-satellite image matching.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVFT.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875">
                <papertitle>Optimal Feature Transport for Cross-View Image Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Liu Liu, Tong Zhang, and Hongdong Li
              <br>
              AAAI, 2020
              <br>
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_CVFT"> code </a>
              <p></p>
              <p> Motivated by optimal transport, we invent a cross-view feature transport module to bridge the cross-view domain gap.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SAFA.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf">
                <papertitle>Spatial-Aware Feature Aggregation for Cross-View Image based Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Liu Liu, Xin Yu, and Hongdong Li
              <br>
              NeurIPS, 2019
              <br>
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_SAFA"> code </a>
              <p></p>
              <p> A polar transform is introduced to bridge the ground-and-satellite domain gap, which significantly boosts the state-of-the-art cross-view localization performance.
              </p>
            </td>
          </tr>


        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
              <p> Conference Program Committee Member/Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on Computer Vision (ICCV), 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">European Conference on Computer Vision (ECCV), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Asian Conference on Computer Vision (ACCV), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Neural Information Processing Systems (NeurIPS), 2022</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The International Conference on Learning Representations (ICLR), 2021, 2022</li> 
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">AAAI Conference on Artificial Intelligence (AAAI), 2021, 2022, 2023</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Joint Conference on Artificial Intelligence (IJCAI), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on 3D Vision (3DV), 2022</li>

                </ul>
              </p>
              <p> Journal Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Computer Vision (IJCV)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Image Processing (TIP)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The IEEE Robotics and Automation Letters (RAL)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Artificial Intelligence</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Geoscience and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ISPRS Journal of Photogrammetry and Remote Sensing</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Digital Earth</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Knowledge-Based Systems</li>
                    
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        

					
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://shiyujiao.github.io/">shiyu jiao's website</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
