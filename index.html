<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shengjun Tang</title>
  
  <meta name="author" content="Shengjun Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🌐</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shengjun Tang</name>
              </p>
              <p> Dr. Shengjun Tang is currently an Assistant Professor/Associate Researcher at <a href="https://www.szu.edu.cn/">Shenzhen University</a>. Prior to this, he worked as a Research Assistant at <a href="https://www.polyu.edu.hk/lsgi/"> the Department of Land Surveying and Geo-Informatics (LSGI) at The Hong Kong Polytechnic University</a>, and as a Postdoctoral Researcher in the team of Academician Renzhong Guo at Shenzhen University.
	      </p>
              <p>His research interests include 3D reconstruction and scene understanding, laser and visual simultaneous localization and mapping (SLAM), Virtual Geography and modeling theory, and applications. 
              </p>
              <p style="text-align:center">
                <a href="mailto:shengjuntang@szu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/CV_ShengjunTang.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q-fQQqUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/shiyujiao">Github</a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Shengjun%20tang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ShengjuntangCircle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2022: I start an internship at Tencent</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2022: One paper accepted to TPAMI 2022 </li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2022: One paper accepted to CVPR 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">01/2022: I start an internship at Meta Reality Lab Research</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">12/2021: One paper accepted to TPAMI 2022</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">06/2021: I am nominated as one of the CVPR 2021 outstanding reviewer</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2021: One paper accepted to CVPR 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">03/2020: One paper accepted to CVPR 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">11/2019: One paper accepted to AAAI 2020</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">09/2019: One paper accepted to NeurIPS 2019</li>

                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table> -->



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treemodeling.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2">
                <papertitle>面向虚拟地理环境构建的树木模型高保真三维重建方法 </papertitle>
              </a>
              <br>
              王伟玺, 黄鸿盛,杜思齐, 李晓明, 谢林甫, 郭仁忠, <strong>汤圣君*</strong>
              <br>
              遥感学报, 2023
              <br>
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2"> paper </a>  /
              <a href="https://github.com/shiyujiao/HighlyAccurate"> code </a>
              <p></p>
              <p> 本文面向虚拟地理环境高逼真场景构建需求，提出一种基于高精度激光扫描点云数据的树木三维模型高保真仿生重建方法，可实现形态特征保持的树木三维模型自动化重建。
		      
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorclassification.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82">
                <papertitle>超体素随机森林与 LSTM 神经网络联合优化的室内点云高精度分类方法 </papertitle>
              </a>
              <br>
              <strong>汤圣君</strong>, 张韵婕, 李晓明, 姚萌萌, 叶致煌, 李亚鑫, 郭仁忠, 王伟玺
              <br>
              武汉大学学报（信息科学版）, 2023
              <br>
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82"> paper </a>  /
              <p></p>
              <p> 针对现有三维点云数据分割分类方法存在分类目标内部不一致的问题，提出一种超体素随机森林与长短期记忆神经网络(long short-term memory，LSTM)联合优化的室内点云高精度分类方法。
		      
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/3DOF.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2203.14148.pdf">
                <papertitle>Accurate 3-DoF Camera Geo-Localization via Ground-to-Satellite Image Matching </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Liu Liu, Dylan Campbell, Piotr Koniusz, and Hongdong Li
              <br>
              TPAMI, 2022
              <br>
              <a href="https://arxiv.org/pdf/2203.14148.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/IBL"> code </a>
              <p></p>
              <p> We propose projective transform, which (1) compliments polar transform to achieve better coarse localization performance and (2) provides a novel handcrafted method to accurately localize query camera on its matching satellite image.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Sat2Str.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2103.01623.pdf">
                <papertitle>Geometry-Guided Street-View Panorama Synthesis from Satellite Imagery </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Dylan Campbell, Xin Yu, and Hongdong Li
              <br>
              TPAMI, 2022
              <br>
              <a href="https://arxiv.org/pdf/2103.01623.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/Sat2StrPanoramaSynthesis"> code </a>
              <p></p>
              <p> Satellite to street-view panorama synthesis, implicit satellite image height map estimation.
              </p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SVNVS.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Self-Supervised_Visibility_Learning _for_Novel_View_Synthesis_CVPR_2021_paper.pdf">
                <papertitle>Self-Supervised Visibility Learning for Novel View Synthesis </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Hongdong Li, and Xin Yu
              <br>
              CVPR, 2021
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Shi_Self-Supervised_Visibility_Learning_for_Novel_View_Synthesis_CVPR_2021_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/SVNVS"> code </a>
              <p></p>
              <p> We estimate target-view depth and source-view visibility in an end-to-end manner.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/DSM.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf">
                <papertitle>Where am I looking at? Joint Location and Orientation Estimation by Cross-View Matching </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Dylan Campbell, and Hongdong Li
              <br>
              CVPR, 2020
              <br>
              <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Shi_Where_Am_I_Looking_At_Joint_Location_and_Orientation_Estimation_CVPR_2020_paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_DSM"> code </a>
              <p></p>
              <p> The first 3-DoF camera pose estimation framework via ground-to-satellite image matching.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CVFT.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875">
                <papertitle>Optimal Feature Transport for Cross-View Image Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Xin Yu, Liu Liu, Tong Zhang, and Hongdong Li
              <br>
              AAAI, 2020
              <br>
              <a href="https://ojs.aaai.org//index.php/AAAI/article/view/6875"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_CVFT"> code </a>
              <p></p>
              <p> Motivated by optimal transport, we invent a cross-view feature transport module to bridge the cross-view domain gap.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/SAFA.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf">
                <papertitle>Spatial-Aware Feature Aggregation for Cross-View Image based Geo-Localization </papertitle>
              </a>
              <br>
              <strong>Yujiao Shi </strong>, Liu Liu, Xin Yu, and Hongdong Li
              <br>
              NeurIPS, 2019
              <br>
              <a href="https://proceedings.neurips.cc/paper/2019/file/ba2f0015122a5955f8b3a50240fb91b2-Paper.pdf"> paper </a>  /
              <a href="https://github.com/shiyujiao/cross_view_localization_SAFA"> code </a>
              <p></p>
              <p> A polar transform is introduced to bridge the ground-and-satellite domain gap, which significantly boosts the state-of-the-art cross-view localization performance.
              </p>
            </td>
          </tr>


        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
              <p> Conference Program Committee Member/Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on Computer Vision (ICCV), 2021</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">European Conference on Computer Vision (ECCV), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Asian Conference on Computer Vision (ACCV), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Neural Information Processing Systems (NeurIPS), 2022</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The International Conference on Learning Representations (ICLR), 2021, 2022</li> 
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">AAAI Conference on Artificial Intelligence (AAAI), 2021, 2022, 2023</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Joint Conference on Artificial Intelligence (IJCAI), 2020, 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Conference on 3D Vision (3DV), 2022</li>

                </ul>
              </p>
              <p> Journal Reviewer:
              </p>
              <p>
                <ul>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Computer Vision (IJCV)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Image Processing (TIP)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The IEEE Robotics and Automation Letters (RAL)</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Artificial Intelligence</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">IEEE Transactions on Geoscience and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">ISPRS Journal of Photogrammetry and Remote Sensing</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">International Journal of Digital Earth</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Knowledge-Based Systems</li>
                    
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        

					
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://shiyujiao.github.io/">shiyu jiao's website</a>.
          </font>
        </p>
      </td>
    </tr>
  </table>
</body>

</html>
